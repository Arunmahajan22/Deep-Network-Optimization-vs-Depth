{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "UCS761 ‚Äì Deep Learning Lab 6\n",
        "Deep Network Structural Stress Test\n",
        "Roll Number: 102303346"
      ],
      "metadata": {
        "id": "C0OPTLCHut3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART A: Deep Network Structural Stress Test\n",
        "Step 1: Extract a, b, c\n",
        "\n",
        "Last three digits of roll number: 346\n",
        "\n",
        "Therefore:\n",
        "\n",
        "* a = 6\n",
        "\n",
        "* b = 4\n",
        "\n",
        "* c = 3"
      ],
      "metadata": {
        "id": "0cEH-PvNuvKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROLL_NUMBER = 102303346\n",
        "\n",
        "roll = str(ROLL_NUMBER)\n",
        "\n",
        "a = int(roll[-1])   # 6\n",
        "b = int(roll[-2])   # 4\n",
        "c = int(roll[-3])   # 3\n",
        "\n",
        "print(\"a =\", a)\n",
        "print(\"b =\", b)\n",
        "print(\"c =\", c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FwtAwVAu6rS",
        "outputId": "25c66841-f93c-4ab2-8f25-7b4b544e92c8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a = 6\n",
            "b = 4\n",
            "c = 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the rules:\n",
        "\n",
        "Hidden layer width = 6 + a = 6 + 6 = 12\n",
        "\n",
        "\n",
        "Number of hidden layers = 4 + (b mod 3)\n",
        "= 4 + (4 mod 3)\n",
        "= 4 + 1\n",
        "= 5 hidden layers\n",
        "\n",
        "\n",
        "Learning rate = 0.002 √ó (c + 1)\n",
        "= 0.002 √ó 4\n",
        "= 0.008\n",
        "\n",
        "\n",
        "\n",
        "Activation:\n",
        "\n",
        "a = 6 (even) ‚Üí ReLU\n",
        "\n",
        "\n",
        "Weight initialization range:\n",
        "\n",
        "[-1/(a+1), +1/(a+1)]\n",
        "= [-1/7, +1/7]\n",
        "\n",
        "Bias = 0"
      ],
      "metadata": {
        "id": "NmtBkpEmvE8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "hidden_width = 6 + a\n",
        "num_hidden_layers = 4 + (b % 3)\n",
        "learning_rate = 0.002 * (c + 1)\n",
        "activation_name = \"relu\"\n",
        "init_range = 1/(a+1)\n",
        "\n",
        "print(hidden_width, num_hidden_layers, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHOvVDU9u9t5",
        "outputId": "cca0905a-f7d3-44d4-9407-4ae150735106"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12 5 0.008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Construction (Nonlinear)\n",
        "\n",
        "We construct nonlinear regression data to force representation bending."
      ],
      "metadata": {
        "id": "tiEJtwpwvaqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(np.random.randint(0,1000))\n",
        "\n",
        "N = 400\n",
        "X = np.random.uniform(-2,2,(N,3))\n",
        "\n",
        "y = (\n",
        "    np.sin(X[:,0]) +\n",
        "    0.5*(X[:,1]**2) -\n",
        "    0.8*X[:,2]\n",
        ").reshape(-1,1)"
      ],
      "metadata": {
        "id": "J2LIQcyOvb4-"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions"
      ],
      "metadata": {
        "id": "g7JOAFmZvlGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    return np.maximum(0,z)\n",
        "\n",
        "def relu_grad(z):\n",
        "    return (z>0).astype(float)"
      ],
      "metadata": {
        "id": "-DQtM4d6vl6I"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Deep Network\n",
        "\n",
        "Architecture:\n",
        "\n",
        "3 ‚Üí 12 ‚Üí 12 ‚Üí 12 ‚Üí 12 ‚Üí 12 ‚Üí 1\n",
        "(5 hidden layers)"
      ],
      "metadata": {
        "id": "WZbRxVpbvrgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [3] + [hidden_width]*num_hidden_layers + [1]\n",
        "\n",
        "weights = []\n",
        "biases = []\n",
        "\n",
        "for i in range(len(layers)-1):\n",
        "    W = np.random.uniform(-init_range, init_range,\n",
        "                          (layers[i], layers[i+1]))\n",
        "    b_vec = np.zeros((1,layers[i+1]))\n",
        "    weights.append(W)\n",
        "    biases.append(b_vec)"
      ],
      "metadata": {
        "id": "FkQ27z5Gvunn"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward and Backward Propagation"
      ],
      "metadata": {
        "id": "ACtkOiIOvxUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X):\n",
        "    A = X\n",
        "    activations = [A]\n",
        "    Zs = []\n",
        "\n",
        "    for i in range(len(weights)-1):\n",
        "        Z = A @ weights[i] + biases[i]\n",
        "        Zs.append(Z)\n",
        "        A = relu(Z)\n",
        "        activations.append(A)\n",
        "\n",
        "    Z_final = A @ weights[-1] + biases[-1]\n",
        "    Zs.append(Z_final)\n",
        "    activations.append(Z_final)\n",
        "\n",
        "    return activations, Zs\n",
        "\n",
        "\n",
        "def backward(activations, Zs, y_true):\n",
        "\n",
        "    grads_W = []\n",
        "    grads_b = []\n",
        "\n",
        "    y_hat = activations[-1]\n",
        "    error = y_hat - y_true\n",
        "    dA = 2*error/len(y_true)\n",
        "\n",
        "    for i in reversed(range(len(weights))):\n",
        "        A_prev = activations[i]\n",
        "        Z = Zs[i]\n",
        "\n",
        "        if i == len(weights)-1:\n",
        "            dZ = dA\n",
        "        else:\n",
        "            dZ = dA * relu_grad(Z)\n",
        "\n",
        "        dW = A_prev.T @ dZ\n",
        "        db = np.sum(dZ,axis=0,keepdims=True)\n",
        "\n",
        "        grads_W.insert(0,dW)\n",
        "        grads_b.insert(0,db)\n",
        "\n",
        "        dA = dZ @ weights[i].T\n",
        "\n",
        "    return grads_W, grads_b"
      ],
      "metadata": {
        "id": "njsxbdsAvyYj"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline Run ‚Äì 400 Epochs"
      ],
      "metadata": {
        "id": "YIFYQfGKv4NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_record = {}\n",
        "\n",
        "for epoch in range(1,401):\n",
        "\n",
        "    activations, Zs = forward(X)\n",
        "    y_hat = activations[-1]\n",
        "\n",
        "    loss = np.mean((y_hat-y)**2)\n",
        "\n",
        "    grads_W, grads_b = backward(activations, Zs, y)\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        weights[i] -= learning_rate * grads_W[i]\n",
        "        biases[i] -= learning_rate * grads_b[i]\n",
        "\n",
        "    if epoch in [1,100,400]:\n",
        "        loss_record[epoch] = loss\n",
        "\n",
        "grad_first = np.linalg.norm(grads_W[0])\n",
        "grad_last = np.linalg.norm(grads_W[-2])\n",
        "GRI = grad_first/grad_last\n",
        "\n",
        "print(\"Loss Epoch1:\", loss_record[1])\n",
        "print(\"Loss Epoch100:\", loss_record[100])\n",
        "print(\"Loss Epoch400:\", loss_record[400])\n",
        "print(\"Grad First:\", grad_first)\n",
        "print(\"Grad Last:\", grad_last)\n",
        "print(\"GRI:\", GRI)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9wr2cAfv6oL",
        "outputId": "c27bcfd0-93b5-4970-8539-0ab5e555dcf5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss Epoch1: 2.108921628085988\n",
            "Loss Epoch100: 1.6939143100570506\n",
            "Loss Epoch400: 1.6792283228593363\n",
            "Grad First: 0.0015464389527515316\n",
            "Grad Last: 0.001269362727289785\n",
            "GRI: 1.2182797867819324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structural Diagnosis (Run 1)\n",
        "\n",
        "\n",
        "Loss decreased from 2.108 (epoch 1) to 1.679 (epoch 400).\n",
        "Training stabilized without oscillation or divergence.\n",
        "\n",
        "Gradient norm (first hidden layer) = 0.00154\n",
        "Gradient norm (last hidden layer) =  0.00126\n",
        "\n",
        "GRI = 1.218\n",
        "\n",
        "Since GRI > 1, the first-layer gradient is slightly larger than the last-layer gradient.\n",
        "\n",
        "This indicates:\n",
        "\n",
        "No vanishing gradient\n",
        "\n",
        "No exploding gradient\n",
        "\n",
        "Gradient flow is relatively stable across depth\n",
        "\n",
        "Early layers are receiving sufficient training signal\n",
        "\n",
        "Therefore, this is not a representation failure and not an optimization instability.\n",
        "\n",
        "The slow loss reduction suggests moderate optimization difficulty but not structural breakdown."
      ],
      "metadata": {
        "id": "W6hyg7qmwEPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forced Structural Break\n",
        "\n",
        "Since:\n",
        "\n",
        "b = 4\n",
        "b mod 3 = 1\n",
        "\n",
        "Rule says:\n",
        "\n",
        "Add +3 hidden layers.\n",
        "\n",
        "New hidden layers = 8"
      ],
      "metadata": {
        "id": "pTIXa6TjwLbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(3):\n",
        "    W = np.random.uniform(-init_range,init_range,\n",
        "                          (hidden_width,hidden_width))\n",
        "    b_vec = np.zeros((1,hidden_width))\n",
        "    weights.insert(-1,W)\n",
        "    biases.insert(-1,b_vec)"
      ],
      "metadata": {
        "id": "pGSubzJewOS1"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run 2 ‚Äì 400 Epochs"
      ],
      "metadata": {
        "id": "o2oF5AtAwSNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_record2 = {}\n",
        "\n",
        "for epoch in range(1,401):\n",
        "\n",
        "    activations, Zs = forward(X)\n",
        "    y_hat = activations[-1]\n",
        "\n",
        "    loss = np.mean((y_hat-y)**2)\n",
        "\n",
        "    grads_W, grads_b = backward(activations, Zs, y)\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        weights[i] -= learning_rate * grads_W[i]\n",
        "        biases[i] -= learning_rate * grads_b[i]\n",
        "\n",
        "    if epoch in [1,100,400]:\n",
        "        loss_record2[epoch] = loss\n",
        "\n",
        "grad_first2 = np.linalg.norm(grads_W[0])\n",
        "grad_last2 = np.linalg.norm(grads_W[-2])\n",
        "GRI2 = grad_first2/grad_last2\n",
        "\n",
        "print(\"Run2 Loss Epoch1:\", loss_record2[1])\n",
        "print(\"Run2 Loss Epoch400:\", loss_record2[400])\n",
        "print(\"Run2 GRI:\", GRI2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMD92-LuwUxJ",
        "outputId": "8f87e7fd-278a-4ee7-b0ea-86d20cb5e3e2"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run2 Loss Epoch1: 1.6807958092530657\n",
            "Run2 Loss Epoch400: 1.679399079753295\n",
            "Run2 GRI: 0.38839788917114976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structural Diagnosis (Run 2)\n",
        "\n",
        "In Run 2, depth increased from 5 to 8 hidden layers.\n",
        "\n",
        "Loss changed from 1.68079 (epoch 1) to 1.67939 (epoch 400), indicating almost no improvement. Training stagnated but did not diverge.\n",
        "\n",
        "GRI reduced significantly from 1.218 (Run 1) to 0.3883 (Run 2).\n",
        "\n",
        "Since GRI < 1, the first-layer gradient is now smaller than the last-layer gradient. This confirms that early-layer gradients shrank relative to later layers.\n",
        "\n",
        "The depth increase caused additional gradient attenuation due to repeated Jacobian multiplications during backpropagation.\n",
        "\n",
        "There was no oscillation or explosion, so learning rate overshoot was not the issue.\n",
        "\n",
        "This is not a representation failure, because the deeper model has higher representational capacity.\n",
        "\n",
        "Instead, this is an optimization instability caused by depth-induced gradient attenuation."
      ],
      "metadata": {
        "id": "yzs16V1UwgTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did GRI increase, decrease, or collapse?\n",
        "\n",
        "It decreased significantly:\n",
        "\n",
        "1.218 ‚Üí 0.3883\n",
        "\n",
        "That is a structural shift toward gradient attenuation."
      ],
      "metadata": {
        "id": "U30RtyYD0llI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did early-layer gradients shrink relative to later layers?\n",
        "\n",
        "Yes.\n",
        "\n",
        "In Run 1:\n",
        "\n",
        "First layer > Last layer\n",
        "\n",
        "In Run 2:\n",
        "\n",
        "First layer < Last layer\n",
        "\n",
        "This confirms gradient decay across depth."
      ],
      "metadata": {
        "id": "Y4vtutlQ0q7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did loss stabilize, oscillate, or diverge?\n",
        "\n",
        "It stabilized almost immediately and barely improved.\n",
        "\n",
        "So:\n",
        "\n",
        "Training stagnated."
      ],
      "metadata": {
        "id": "06kqXM7H013k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Was failure due to:\n",
        "\n",
        "Depth multiplication? YES\n",
        "\n",
        "Activation slope behavior? Minor factor\n",
        "\n",
        "Learning rate overshoot? NO\n",
        "\n",
        "Learning rate was unchanged (0.008).\n",
        "No explosion occurred.\n",
        "\n",
        "The key change was depth."
      ],
      "metadata": {
        "id": "RJLjrc9k1E_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representation failure or optimization instability?\n",
        "\n",
        "Important distinction:\n",
        "\n",
        "The model is more expressive in Run 2.\n",
        "\n",
        "So it is not representation failure.\n",
        "\n",
        "It is:\n",
        "\n",
        "Optimization difficulty caused by gradient attenuation due to increased depth."
      ],
      "metadata": {
        "id": "N3iN0LVS1Q_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART B ‚Äì Structural Reading Component"
      ],
      "metadata": {
        "id": "llz6Bkh5wlW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dense vs Convolution Parameter Comparison\n",
        "\n",
        "Given:\n",
        "\n",
        "a = 6\n",
        "b = 4\n",
        "c = 3\n",
        "\n",
        "Input size = (24 + a) = 30\n",
        "Dense hidden neurons = 32 + b = 36\n",
        "\n",
        "Dense parameters:\n",
        "\n",
        "(30√ó30) √ó 36 + 36\n",
        "= 900 √ó 36 + 36\n",
        "= 32400 + 36\n",
        "= 32436"
      ],
      "metadata": {
        "id": "sWfkG0MCwof9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense_params = (30*30)*36 + 36\n",
        "dense_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o82g2nEwqlU",
        "outputId": "ac89308f-53ea-4c9a-8da6-4fdb26b20d0f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32436"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution\n",
        "\n",
        "Filter size = 3 + (c mod 2)\n",
        "= 3 + 1 = 4\n",
        "\n",
        "Filters = 8 + a = 14\n",
        "\n",
        "Conv parameters:\n",
        "\n",
        "(4√ó4)√ó14 + 14\n",
        "= 16√ó14 +14\n",
        "= 224 +14\n",
        "= 238"
      ],
      "metadata": {
        "id": "qx13zaBvwvTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_params = (4*4)*14 + 14\n",
        "conv_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HskHqwBiwydF",
        "outputId": "2502ff73-72e9-434c-cb27-07ca527c7427"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "238"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why scaling differs?\n",
        "\n",
        "Dense connects every pixel to every neuron.\n",
        "Parameter count grows quadratically with image size.\n",
        "\n",
        "Convolution uses local receptive fields and weight sharing.\n",
        "Parameter count depends only on filter size, not full image size."
      ],
      "metadata": {
        "id": "yFHjWh1Cw1sI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output Size Calculation\n",
        "\n",
        "N = 30\n",
        "F = 3\n",
        "S = 1 + (b mod 2) = 1 + 0 = 1\n",
        "P = (c mod 2) = 1\n",
        "\n",
        "Output size formula:\n",
        "\n",
        "((N ‚àí F + 2P) / S) + 1\n",
        "\n",
        "= (30 ‚àí 3 + 2)/1 + 1\n",
        "= 29 + 1\n",
        "= 30"
      ],
      "metadata": {
        "id": "FtZeG1Pgw5Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_size = (30-3+2)/1 + 1\n",
        "output_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIrdjJujw7JE",
        "outputId": "932c9ec2-6139-462c-b454-a3d342d4d533"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.0"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual Convolution (Center Only)\n",
        "\n",
        "Construct matrix:\n",
        "\n",
        "value(i,j) = (a+i) + (b+j)\n",
        "\n",
        "a=6, b=4\n",
        "\n",
        "Center 3√ó3 region computed manually.\n",
        "\n",
        "Filter:\n",
        "\n",
        "Center = +3\n",
        "All others = ‚àí1\n",
        "\n",
        "After elementwise multiplication and summation:\n",
        "\n",
        "Final center convolution output = -18"
      ],
      "metadata": {
        "id": "CFIRwLyzw_RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = np.zeros((5,5))\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        matrix[i,j] = (6+i) + (4+j)\n",
        "\n",
        "filter_mat = -1*np.ones((3,3))\n",
        "filter_mat[1,1] = 3\n",
        "\n",
        "sub = matrix[1:4,1:4]\n",
        "result = np.sum(sub*filter_mat)\n",
        "\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjty2Cl6xB2y",
        "outputId": "9ba753ad-9687-4ee4-c993-c8a09c2b726e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(-70.0)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1Ô∏è Based on your recorded GRI, if your first-layer gradient was X and last-layer gradient was Y, what does that mathematically say about signal survival across your network depth?\n",
        "Run 1\n",
        "\n",
        "First-layer gradient (X) = 0.00154\n",
        "Last-layer gradient (Y) = 0.00126\n",
        "\n",
        "GRI= 1.218\n",
        "\n",
        "Since GRI > 1, the gradient magnitude in the first hidden layer is actually larger than in the last hidden layer.\n",
        "\n",
        "This mathematically indicates:\n",
        "\n",
        "Gradient signal did not vanish.\n",
        "\n",
        "Backpropagated signal was preserved across depth.\n",
        "\n",
        "There was no early-layer starvation of learning signal.\n",
        "\n",
        "Signal survival across 5 hidden layers was stable and even slightly amplified.\n",
        "\n",
        "Run 2\n",
        "\n",
        "GRI = 0.3883\n",
        "\n",
        "Now:\n",
        "\n",
        "‚Äñ\n",
        "‚àá\n",
        "ùëä\n",
        "ùëì\n",
        "ùëñ\n",
        "ùëü\n",
        "ùë†\n",
        "ùë°\n",
        "‚Äñ\n",
        "<\n",
        "‚Äñ\n",
        "‚àá\n",
        "ùëä\n",
        "ùëô\n",
        "ùëé\n",
        "ùë†\n",
        "ùë°\n",
        "‚Äñ\n",
        "‚Äñ‚àáW\n",
        "first\n",
        "\t‚Äã\n",
        "\n",
        "‚Äñ<‚Äñ‚àáW\n",
        "last\n",
        "\t‚Äã\n",
        "\n",
        "‚Äñ\n",
        "\n",
        "This means the gradient magnitude at the first layer is only 36.85% of the last hidden layer's gradient.\n",
        "\n",
        "Mathematically, this confirms:\n",
        "\n",
        "Gradient attenuation occurred due to increased depth.\n",
        "\n",
        "Signal decayed while propagating backward.\n",
        "\n",
        "Early layers received significantly weaker updates.\n",
        "\n",
        "Thus, depth directly reduced gradient survival."
      ],
      "metadata": {
        "id": "inugkZ4bxFbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2Ô∏è If I remove all activation functions from your architecture but keep the same depth and parameters, how many effective linear layers remain? What does that imply about representation power?\n",
        "\n",
        "Without activation functions, each layer becomes a linear transformation:\n",
        "\n",
        "ùëç\n",
        "L\n",
        "+\n",
        "1\n",
        ":\n",
        "ùëä\n",
        "L\n",
        "*\n",
        "ùëç\n",
        "L\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Stacking linear transformations results in:\n",
        "\n",
        "ùëä\n",
        "5\n",
        "ùëä\n",
        "4\n",
        "ùëä\n",
        "3\n",
        "ùëä\n",
        "2\n",
        "ùëä\n",
        "1\n",
        "ùëã\n",
        "W\n",
        "5\n",
        "\t‚Äã\n",
        "\n",
        "W\n",
        "4\n",
        "\t‚Äã\n",
        "\n",
        "W\n",
        "3\n",
        "\t‚Äã\n",
        "\n",
        "W\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "W\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "X\n",
        "\n",
        "This collapses into a single matrix multiplication:\n",
        "\n",
        "ùëä\n",
        "~\n",
        "ùëã\n",
        "W\n",
        "~\n",
        "X\n",
        "\n",
        "So effective linear layers = 1\n",
        "\n",
        "Implication:\n",
        "\n",
        "Depth provides zero additional representational power.\n",
        "\n",
        "The network reduces to a single linear regression model.\n",
        "\n",
        "No nonlinear bending occurs.\n",
        "\n",
        "It cannot model sine or quadratic curvature in the dataset.\n",
        "\n",
        "Therefore, activation functions are essential for representation power."
      ],
      "metadata": {
        "id": "g2f_tapfxIcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3Ô∏è What was one assumption you had about deep networks that your own experiment proved wrong?\n",
        "\n",
        "I initially assumed:\n",
        "\n",
        "Increasing depth always improves learning performance.\n",
        "\n",
        "However, in my experiment:\n",
        "\n",
        "Run 1 (5 layers) had stable gradient flow (GRI = 1.22=18).\n",
        "\n",
        "Run 2 (8 layers) had attenuated gradient flow (GRI = 0.3883).\n",
        "\n",
        "Loss improvement nearly stopped in Run 2.\n",
        "\n",
        "This showed that deeper networks can introduce optimization difficulty even when representational capacity increases.\n",
        "\n",
        "Thus, depth alone does not guarantee better training performance."
      ],
      "metadata": {
        "id": "BGCr8RucxMmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4Ô∏è If GPT had generated this architecture for you without your roll-number constraint, what structural mismatch would immediately expose that it was not following the assignment rules?\n",
        "\n",
        "For roll number 102303346:\n",
        "\n",
        "Hidden width must be 12.\n",
        "\n",
        "Hidden layers must be 5.\n",
        "\n",
        "Learning rate must be 0.008.\n",
        "\n",
        "Activation must be ReLU.\n",
        "\n",
        "Initialization range must be [-1/7, +1/7].\n",
        "\n",
        "If GPT produced:\n",
        "\n",
        "10 hidden units\n",
        "\n",
        "4 layers instead of 5\n",
        "\n",
        "Learning rate 0.01\n",
        "\n",
        "Sigmoid activation\n",
        "\n",
        "Initialization [-0.5,0.5]\n",
        "\n",
        "That would immediately expose non-compliance with the roll-number constraints.\n",
        "\n",
        "Since all architectural components depend mathematically on (a,b,c), any mismatch would reveal structural inconsistency."
      ],
      "metadata": {
        "id": "zkjFN_IoxQkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5 If your gradients could ‚Äútalk‚Äù during Run 2, what would the first layer complain about, and why?\n",
        "\n",
        "During Run 2, depth increased to 8 hidden layers.\n",
        "\n",
        "GRI dropped from 1.218 ‚Üí 0.3883.\n",
        "\n",
        "The first layer would complain:\n",
        "\n",
        "Every time the error signal travels backward through another weight matrix and ReLU derivative, my magnitude shrinks. By the time it reaches me, I am only receiving 36% of the last layer's strength.\n",
        "\n",
        "Thus, the first layer receives weakened learning signals, slowing parameter updates."
      ],
      "metadata": {
        "id": "RsROoCK9xTdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_history = []\n",
        "\n",
        "for epoch in range(1,401):\n",
        "\n",
        "    activations, Zs = forward(X)\n",
        "    y_hat = activations[-1]\n",
        "\n",
        "    loss = np.mean((y_hat - y)**2)\n",
        "    loss_history.append(loss)\n",
        "\n",
        "    grads_W, grads_b = backward(activations, Zs, y)\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        weights[i] -= learning_rate * grads_W[i]\n",
        "        biases[i] -= learning_rate * grads_b[i]\n",
        "\n",
        "# Create DataFrame\n",
        "df_loss = pd.DataFrame({\n",
        "    \"Epoch\": range(1,401),\n",
        "    \"Loss\": loss_history\n",
        "})\n",
        "\n",
        "# Show specific epoch\n",
        "df_loss[df_loss[\"Epoch\"] == 257]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "pvbDkmYFxVEx",
        "outputId": "57809f09-26f5-445d-82db-491f5560cc95"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Epoch      Loss\n",
              "256    257  1.679399"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50deef0f-8b33-4119-bbcc-e137061d48e7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>257</td>\n",
              "      <td>1.679399</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50deef0f-8b33-4119-bbcc-e137061d48e7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-50deef0f-8b33-4119-bbcc-e137061d48e7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-50deef0f-8b33-4119-bbcc-e137061d48e7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_loss[df_loss[\\\"Epoch\\\"] == 257]\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 257,\n        \"max\": 257,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          257\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.6793990735217834,\n        \"max\": 1.6793990735217834,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.6793990735217834\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    }
  ]
}